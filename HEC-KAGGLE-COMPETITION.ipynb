{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ab56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "datrain = pd.read_csv(\"train.csv\")\n",
    "datest= pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datrain[\"y\"] = datrain[\"y\"].astype(\"category\")\n",
    "datrain[\"country\"] = datrain[\"country\"].astype(\"category\")\n",
    "datest[\"country\"] = datest[\"country\"].astype(\"category\")\n",
    "\n",
    "\n",
    "#datrain[\"difflevel\"] = datrain[\"difflevel\"].astype(\"category\")\n",
    "#datest[\"difflevel\"] = datest[\"difflevel\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149961d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_train, train_val = train_test_split(datrain, train_size=0.7, stratify=datrain['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de525519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31aec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en características (X) y variable objetivo (y)\n",
    "X_train = train_train.drop(['y'], axis=1)\n",
    "y_train = train_train['y']\n",
    "X_val = train_val.drop(['y'], axis=1)\n",
    "y_val = train_val['y']\n",
    "\n",
    "# Convertir las variables categóricas en numéricas utilizando codificación one-hot\n",
    "categorical_features = ['country']\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_val = ct.transform(X_val)\n",
    "\n",
    "# Normalizar los datos utilizando StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5b419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e523566",
   "metadata": {},
   "source": [
    "# boosting catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5db4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pool = Pool(data=X_train, label=y_train)\n",
    "val_pool = Pool(data=X_val, label=y_val)\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggérer des valeurs pour les hyperparamètres\n",
    "    depth = trial.suggest_int('depth', 2, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True)\n",
    "    random_strength = trial.suggest_float('random_strength', 1e-8, 10.0, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.2, 1)\n",
    "    bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bernoulli', 'MVS'])\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=learning_rate,\n",
    "        depth=depth,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        random_strength=random_strength,\n",
    "        subsample=subsample,\n",
    "        bootstrap_type=bootstrap_type,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='MultiClass',\n",
    "        random_seed=1,\n",
    "        verbose=False,  # silence training\n",
    "        early_stopping_rounds=100,\n",
    "        task_type='CPU'\n",
    "    )\n",
    "    \n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "sampler = TPESampler(seed=1)\n",
    "study_cb = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_cb.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_cb.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_cb.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les meilleurs hyperparamètres trouvés\n",
    "best_params_CatBoost = study_cb.best_params\n",
    "\n",
    "# Créer un modèle CatBoost avec ces hyperparamètres\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=best_params_CatBoost['learning_rate'],\n",
    "    depth=best_params_CatBoost['depth'],\n",
    "    l2_leaf_reg=best_params_CatBoost['l2_leaf_reg'],\n",
    "    random_strength=best_params_CatBoost['random_strength'],\n",
    "    subsample=best_params_CatBoost['subsample'],\n",
    "    bootstrap_type=best_params_CatBoost['bootstrap_type'],\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='MultiClass',\n",
    "    random_seed=1,\n",
    "    verbose=200,  # Afficher les logs chaque 200 itérations\n",
    "    early_stopping_rounds=100,\n",
    "    task_type='CPU'\n",
    ")\n",
    "\n",
    "# Entraîner le modèle avec les données d'entraînement\n",
    "model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "# Prédire sur l'ensemble de validation\n",
    "y_pred_CatBoost = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_CatBoost)\n",
    "print(f\"Accuracy (Taux de bonne classification avec les meilleurs hyperparamètres): {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00199666",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d72168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Préparer l'ensemble complet de données d'entraînement (datrain) et les données de test (datest).\n",
    "\n",
    "X_full_train = datrain.drop(['y'], axis=1)\n",
    "y_full_train = datrain['y']\n",
    "\n",
    "# 2. Convertisser les variables catégoriques en numériques en utilisant la codification one-hot.\n",
    "\n",
    "# Ajuster le transformateur sur les données d'entraînement complètes et les transformer\n",
    "X_full_train_encoded = ct.fit_transform(X_full_train)\n",
    "\n",
    "datest_1 = datest.drop('id', axis=1)\n",
    "# Transformer les données de test\n",
    "X_test_encoded = ct.transform(datest_1)\n",
    "\n",
    "# Créer des pools pour l'entraînement et le test\n",
    "full_train_pool = Pool(data=X_full_train_encoded, label=y_full_train)\n",
    "test_pool = Pool(data=X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle avec les données d'entraînement\n",
    "model_full = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=best_params_CatBoost['learning_rate'],\n",
    "    depth=best_params_CatBoost['depth'],\n",
    "    l2_leaf_reg=best_params_CatBoost['l2_leaf_reg'],\n",
    "    random_strength=best_params_CatBoost['random_strength'],\n",
    "    subsample=best_params_CatBoost['subsample'],\n",
    "    bootstrap_type=best_params_CatBoost['bootstrap_type'],\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='MultiClass',\n",
    "    random_seed=1,\n",
    "    verbose=200,\n",
    "    task_type='CPU'\n",
    ")\n",
    "\n",
    "model_full.fit(full_train_pool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8db8d4",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "y_test_pred = model_full.predict(test_pool)\n",
    "CatBoost_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_test_pred.flatten()})\n",
    "CatBoost_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d9ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CatBoost_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed67d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "CatBoost_model_soumission.to_csv(pa + \"CatBoost_model_soumission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c03a1",
   "metadata": {},
   "source": [
    "# boosting lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b594e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggérer des valeurs pour les hyperparamètres\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 2, 650)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
    "    min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "    feature_fraction = trial.suggest_float(\"feature_fraction\", 0.4, 1.0)\n",
    "    bagging_fraction = trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "    bagging_freq = trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "    lambda_l1 = trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True)\n",
    "    lambda_l2 = trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True)\n",
    "\n",
    "    classifier = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt',\n",
    "        objective='multiclass',\n",
    "        metric='multi_logloss',\n",
    "        num_class=4,\n",
    "        num_leaves=num_leaves,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_samples=min_child_samples,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction=bagging_fraction,\n",
    "        bagging_freq=bagging_freq,\n",
    "        lambda_l1=lambda_l1,\n",
    "        lambda_l2=lambda_l2,\n",
    "        random_seed=1\n",
    "    )\n",
    "\n",
    "    classifier.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0, early_stopping_rounds=100)\n",
    "    \n",
    "    y_pred = classifier.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=1)\n",
    "study_lg = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_lg.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_lg.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_lg.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = trial.params\n",
    "best_params_lg = study_lg.best_params\n",
    "\n",
    "best_lgbm = lgb.LGBMClassifier(\n",
    "    num_leaves=best_params_lg[\"num_leaves\"],\n",
    "    learning_rate=best_params_lg[\"learning_rate\"],\n",
    "    min_child_samples=best_params_lg[\"min_child_samples\"],\n",
    "    feature_fraction=best_params_lg[\"feature_fraction\"],\n",
    "    bagging_fraction=best_params_lg[\"bagging_fraction\"],\n",
    "    bagging_freq=best_params_lg[\"bagging_freq\"],\n",
    "    lambda_l1=best_params_lg[\"lambda_l1\"],\n",
    "    lambda_l2=best_params_lg[\"lambda_l2\"],\n",
    "    random_seed=1\n",
    ")\n",
    "\n",
    "best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val_lgbm = best_lgbm.predict(X_val)\n",
    "accuracy_val_lgbm = accuracy_score(y_val, y_pred_val_lgbm)\n",
    "print(f\"Accuracy of LGBM on validation set: {accuracy_val_lgbm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a97fc",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_lgbm = lgb.LGBMClassifier(\n",
    "    num_leaves=best_params_lg[\"num_leaves\"],\n",
    "    learning_rate=best_params_lg[\"learning_rate\"],\n",
    "    min_child_samples=best_params_lg[\"min_child_samples\"],\n",
    "    feature_fraction=best_params_lg[\"feature_fraction\"],\n",
    "    bagging_fraction=best_params_lg[\"bagging_fraction\"],\n",
    "    bagging_freq=best_params_lg[\"bagging_freq\"],\n",
    "    lambda_l1=best_params_lg[\"lambda_l1\"],\n",
    "    lambda_l2=best_params_lg[\"lambda_l2\"],\n",
    "    random_seed=1\n",
    ")\n",
    "\n",
    "model_full_lgbm.fit(X_full_train_encoded, y_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2072a0d",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f929e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "y_test_pred = model_full_lgbm.predict(X_test_encoded)\n",
    "Lgb_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_test_pred.flatten()})\n",
    "Lgb_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lgb_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6880d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "Lgb_model_soumission.to_csv(pa + \"Lgb_model_soumission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f369b8e",
   "metadata": {},
   "source": [
    "# boosting xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80370d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Conversion des labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Définition des hyperparamètres à optimiser\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1, step=0.1),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_\": trial.suggest_float(\"lambda_\", 1e-8, 10.0, log=True)\n",
    "    }\n",
    "\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        reg_alpha=params[\"alpha\"],\n",
    "        reg_lambda=params[\"lambda_\"],\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(le.classes_),\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    xgb_classifier.fit(X_train_scaled, y_train_encoded, eval_set=[(X_val_scaled, y_val_encoded)], verbose=False, early_stopping_rounds=50)\n",
    "    \n",
    "    y_pred_encoded = xgb_classifier.predict(X_val_scaled)\n",
    "    y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "sampler_xgb = optuna.samplers.TPESampler(seed=1)\n",
    "study_xgb = optuna.create_study(direction=\"maximize\", sampler=sampler_xgb)\n",
    "study_xgb.optimize(objective_xgb, n_trials=100)  # Ajustez n_trials selon les besoins\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_xgb.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_xgb.best_trial\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = trial.params\n",
    "best_params_xgb = study_xgb.best_params\n",
    "\n",
    "best_xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=best_params_xgb[\"n_estimators\"],\n",
    "    max_depth=best_params_xgb[\"max_depth\"],\n",
    "    learning_rate=best_params_xgb[\"learning_rate\"],\n",
    "    subsample=best_params_xgb[\"subsample\"],\n",
    "    colsample_bytree=best_params_xgb[\"colsample_bytree\"],\n",
    "    gamma=best_params_xgb[\"gamma\"],\n",
    "    reg_alpha=best_params_xgb[\"alpha\"],\n",
    "    reg_lambda=best_params_xgb[\"lambda_\"],\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(le.classes_),\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "best_xgb_classifier.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "\n",
    "y_val_pred_encoded = best_xgb_classifier.predict(X_val_scaled)\n",
    "y_val_pred = le.inverse_transform(y_val_pred_encoded)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy of Best Model: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fa96c",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best_params_xgb[\"n_estimators\"],\n",
    "    max_depth=best_params_xgb[\"max_depth\"],\n",
    "    learning_rate=best_params_xgb[\"learning_rate\"],\n",
    "    subsample=best_params_xgb[\"subsample\"],\n",
    "    colsample_bytree=best_params_xgb[\"colsample_bytree\"],\n",
    "    gamma=best_params_xgb[\"gamma\"],\n",
    "    reg_alpha=best_params_xgb[\"alpha\"],\n",
    "    reg_lambda=best_params_xgb[\"lambda_\"],\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(le.classes_),\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Conversion des labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_full_train)\n",
    "\n",
    "\n",
    "\n",
    "model_full_xgb.fit(X_full_train_encoded, y_train_encoded)\n",
    "\n",
    "#y_pred_encoded = xgb_classifier.predict(X_val_scaled)\n",
    "   # y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aaf92e",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8eb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "y_pred_encoded = model_full_xgb.predict(X_test_encoded)\n",
    "y_test_pred = le.inverse_transform(y_pred_encoded)\n",
    "xgb_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_test_pred.flatten()})\n",
    "xgb_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "xgb_model_soumission.to_csv(pa + \"xgb_model_soumission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3dd0d6",
   "metadata": {},
   "source": [
    "# GradientBoosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04811d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "\n",
    "# Conversion des labels (si nécessaire)\n",
    "#le = LabelEncoder()\n",
    "#y_train_encoded = le.fit_transform(y_train)\n",
    "#y_val_encoded = le.transform(y_val)\n",
    "\n",
    "def objective_gbc(trial):\n",
    "    # Définition des hyperparamètres à optimiser\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "    }\n",
    "\n",
    "    gbc = GradientBoostingClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        max_features=params[\"max_features\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    gbc.fit(X_train_scaled, y_train)\n",
    "    y_pred = gbc.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "sampler_gbc = optuna.samplers.TPESampler(seed=1)\n",
    "study_gbc = optuna.create_study(direction=\"maximize\", sampler=sampler_gbc)\n",
    "study_gbc.optimize(objective_gbc, n_trials=100)  # Ajustez n_trials selon les besoins\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_gbc.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_gbc.best_trial\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633af1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des meilleurs hyperparamètres\n",
    "best_params_gbc = study_gbc.best_params\n",
    "\n",
    "# Création et entraînement du modèle avec les meilleurs hyperparamètres\n",
    "best_gbc = GradientBoostingClassifier(**best_params_gbc, random_state=1)\n",
    "best_gbc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de validation\n",
    "y_pred = best_gbc.predict(X_val_scaled)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd72c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_gbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e8e50",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et entraînement du modèle avec les meilleurs hyperparamètres\n",
    "model_full_gbc = GradientBoostingClassifier(**best_params_gbc, random_state=1)\n",
    "model_full_gbc.fit(X_full_train_encoded, y_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18369ad6",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "y_test_pred = model_full_gbc.predict(X_test_encoded)\n",
    "gbc_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_test_pred.flatten()})\n",
    "gbc_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "gbc_model_soumission.to_csv(pa + \"gbc_model_soumission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc75dd",
   "metadata": {},
   "source": [
    "# adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0226f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Conversion des labels\n",
    "#le = LabelEncoder()\n",
    "#y_train_encoded = le.fit_transform(y_train)\n",
    "#y_val_encoded = le.transform(y_val)\n",
    "\n",
    "def objective_adaboost(trial):\n",
    "    # Définition des hyperparamètres à optimiser\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 500)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n",
    "    \n",
    "    # Utilisation de DecisionTreeClassifier comme estimateur de base\n",
    "    base_max_depth = trial.suggest_int(\"base_max_depth\", 1, 10)\n",
    "    base_classifier = DecisionTreeClassifier(max_depth=base_max_depth)\n",
    "\n",
    "    classifier = AdaBoostClassifier(\n",
    "        base_estimator=base_classifier,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "sampler_adaboost = optuna.samplers.TPESampler(seed=42)\n",
    "study_adaboost = optuna.create_study(direction=\"maximize\", sampler=sampler_adaboost)\n",
    "study_adaboost.optimize(objective_adaboost, n_trials=100)  # Ajustez n_trials selon les besoins\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_adaboost.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_adaboost.best_trial\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_adaboost = study_adaboost.best_params\n",
    "\n",
    "# Utiliser les meilleurs hyperparamètres \n",
    "best_base_classifier = DecisionTreeClassifier(max_depth=best_params_adaboost[\"base_max_depth\"])\n",
    "best_classifier = AdaBoostClassifier(\n",
    "    base_estimator=best_base_classifier,\n",
    "    n_estimators=best_params_adaboost[\"n_estimators\"],\n",
    "    learning_rate=best_params_adaboost[\"learning_rate\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = best_classifier.predict(X_val_scaled)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy on validation set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b48379",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_adaboost  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2789d10",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe9e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_base_classifier = DecisionTreeClassifier(max_depth=best_params_adaboost[\"base_max_depth\"])\n",
    "model_full_adaboost = AdaBoostClassifier(\n",
    "    base_estimator=best_base_classifier,\n",
    "    n_estimators=best_params_adaboost[\"n_estimators\"],\n",
    "    learning_rate=best_params_adaboost[\"learning_rate\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_full_adaboost.fit(X_full_train_encoded, y_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285875a",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "y_test_pred = model_full_adaboost.predict(X_test_encoded)\n",
    "adaboost_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_test_pred.flatten()})\n",
    "adaboost_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058947",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7321f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "adaboost_model_soumission.to_csv(pa + \"adaboost_model_soumission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d672cb",
   "metadata": {},
   "source": [
    "# réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb18b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    n_neurons = trial.suggest_int('n_neurons', 1, 16)\n",
    "    hidden_layer_sizes = (n_neurons,) * n_layers\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 8, 32)\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    # Créer et entraîner le classificateur\n",
    "    rn_model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        activation='relu',\n",
    "                        solver='adam',\n",
    "                        learning_rate='constant',\n",
    "                        alpha=alpha,\n",
    "                        max_iter=1000,  # réduit pour un essai rapide\n",
    "                        random_state=1,\n",
    "                        verbose=1,\n",
    "                        tol=0.0001,\n",
    "                        batch_size=batch_size,\n",
    "                        n_iter_no_change=20,  # arrêt plus rapide si pas d'amélioration\n",
    "                        learning_rate_init=learning_rate_init)\n",
    "    rn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Évaluer le classificateur\n",
    "    y_pred = rn_model.predict(X_val_scaled)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "study_rn = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_rn.optimize(objective, n_trials=200)  # réduit pour un essai rapide\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_rn.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_rn.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f1603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rn_best_params = study_rn.best_params\n",
    "\n",
    "best_n_layers = rn_best_params['n_layers']\n",
    "best_n_neurons = rn_best_params['n_neurons']\n",
    "best_hidden_layer_sizes = (best_n_neurons,) * best_n_layers\n",
    "best_alpha = rn_best_params['alpha']\n",
    "best_batch_size = rn_best_params['batch_size']\n",
    "best_learning_rate_init = rn_best_params['learning_rate_init']\n",
    "\n",
    "rn_best = MLPClassifier(\n",
    "    hidden_layer_sizes=best_hidden_layer_sizes,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='constant',\n",
    "    alpha=best_alpha,\n",
    "    max_iter=1000,  # 1000 pour un entraînement complet\n",
    "    random_state=1,\n",
    "    verbose=1,\n",
    "    tol=0.0001,\n",
    "    batch_size=best_batch_size,\n",
    "    n_iter_no_change=20,\n",
    "    learning_rate_init=best_learning_rate_init\n",
    ")\n",
    "\n",
    "\n",
    "rn_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_pred_val = rn_best.predict(X_val_scaled)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Accuracy on validation set: {accuracy_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828395d2",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91732744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_full_train_encoded = ct.fit_transform(X_full_train)\n",
    "\n",
    "# Normaliser les données en utilisant StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_full_train_encoded_scaled = scaler.fit_transform(X_full_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9b8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_rn_best = MLPClassifier(\n",
    "    hidden_layer_sizes=best_hidden_layer_sizes,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='constant',\n",
    "    alpha=best_alpha,\n",
    "    max_iter=1000,  # 1000 pour un entraînement complet\n",
    "    random_state=1,\n",
    "    verbose=1,\n",
    "    tol=0.0001,\n",
    "    batch_size=best_batch_size,\n",
    "    n_iter_no_change=20,\n",
    "    learning_rate_init=best_learning_rate_init\n",
    ")\n",
    "\n",
    "\n",
    "full_rn_best.fit(X_full_train_encoded_scaled, y_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09e8a4",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85708baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "\n",
    "X_test_encoded_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "y_pred_test = full_rn_best.predict(X_test_encoded_scaled)\n",
    "\n",
    "\n",
    "rn_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_pred_test.flatten()})\n",
    "rn_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abeb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4034268",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "rn_model_soumission.to_csv(pa + \"rn_model_soumission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a6093",
   "metadata": {},
   "source": [
    "# STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af43ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf5ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# staking 2 modeles avec 100 trials ## el mejor***************************************************\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def objective_stacking(trial):\n",
    "    # Suggérer des hyperparamètres pour le méta-modèle\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "\n",
    "    # Créer le méta-modèle avec les hyperparamètres suggérés\n",
    "    meta_model = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "    \n",
    "    # Noter que les modèles de base sont déjà optimisés\n",
    "    estimators = [('model_catboost', model), ('rn_best', rn_best)]\n",
    "    \n",
    "    stacking_1 = StackingClassifier(estimators=estimators, final_estimator=meta_model)\n",
    "    \n",
    "    stacking_1.fit(X_train_scaled, y_train)\n",
    "    y_pred = stacking_1.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "sampler_stacking = TPESampler(seed=1)\n",
    "study_stacking_1 = optuna.create_study(direction=\"maximize\", sampler=sampler_stacking)\n",
    "study_stacking_1.optimize(objective_stacking, n_trials=100)  # Augmenter le nombre d'essais si nécessaire\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_stacking_1.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial_stacking = study_stacking_1.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial_stacking.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial_stacking.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaae63f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Récupérer les meilleurs hyperparamètres de notre étude Optuna\n",
    "stk1_best_params = study_stacking_1.best_params\n",
    "\n",
    "learning_rate_best = stk1_best_params[\"learning_rate\"]\n",
    "n_estimators_best = stk1_best_params[\"n_estimators\"]\n",
    "max_depth_best = stk1_best_params[\"max_depth\"]\n",
    "\n",
    "# 2. Entraîner le modèle de stacking avec ces hyperparamètres\n",
    "meta_model_best_1 = GradientBoostingClassifier(learning_rate=learning_rate_best, \n",
    "                                             n_estimators=n_estimators_best, \n",
    "                                             max_depth=max_depth_best)\n",
    "\n",
    "estimators_best_1 = [('model_catboost', model), ('rn_best', rn_best)]\n",
    "\n",
    "stacking_best_1 = StackingClassifier(estimators=estimators_best_1, final_estimator=meta_model_best_1)\n",
    "stacking_best_1.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Valider sur l'ensemble de validation\n",
    "y_pred_val = stacking_best_1.predict(X_val_scaled)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Accuracy on validation set:\", accuracy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk1_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afdc6a",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ae728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_stacking_best_1 = StackingClassifier(estimators=estimators_best_1, final_estimator=meta_model_best_1)\n",
    "full_stacking_best_1.fit(X_full_train_encoded_scaled, y_full_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476005f",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e457ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "\n",
    "y_pred_test_stacking_1 = full_stacking_best_1.predict(X_test_encoded_scaled)\n",
    "\n",
    "\n",
    "stacking_1_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_pred_test_stacking_1.flatten()})\n",
    "stacking_1_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_1_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dde824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "stacking_1_model_soumission.to_csv(pa + \"stacking_1_model_soumission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607134a3",
   "metadata": {},
   "source": [
    "# importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ced779",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_feature_names = ct.get_feature_names_out()\n",
    "transformed_feature_names\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(stacking_best_1, random_state=1).fit(X_val_scaled, y_val)\n",
    "display(eli5.show_weights(perm,top=None, feature_names = transformed_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34a849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c75e5818",
   "metadata": {},
   "source": [
    "# réseau de neurones bayésien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "#Convertir les données en tenseurs PyTorch :\n",
    "#X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "#y_train_tensor = torch.tensor(y_train.values, dtype=torch.int64)\n",
    "#X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "#y_val_tensor = torch.tensor(y_val, dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "#y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "#y_train_tensor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Création d'un objet LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Adapter le LabelEncoder aux données et transformer les étiquettes pour commencer à 0\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9699240",
   "metadata": {},
   "source": [
    "# optimisation reseaux  bayésien  avec optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552947eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition du Modèle\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_bayesian_network(n_layers, n_neurons, in_features, out_features):\n",
    "    layers = []\n",
    "    layers.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=in_features, out_features=n_neurons))\n",
    "    layers.append(nn.ReLU())\n",
    "    for _ in range(n_layers - 1):\n",
    "        layers.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=n_neurons, out_features=n_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=n_neurons, out_features=out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BayesianNNWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, n_epochs, batch_size, learning_rate_init, kl_weight, alpha, tol, max_epochs_no_improve):\n",
    "        self.model = model\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.kl_weight = kl_weight\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_epochs_no_improve = max_epochs_no_improve\n",
    "        \n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate_init, weight_decay=self.alpha)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_numpy()  # \n",
    "        y_tensor = torch.tensor(y, dtype=torch.int64)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                pre = self.model(batch_X)\n",
    "                ce = self.ce_loss(pre, batch_y )\n",
    "                kl = self.kl_loss(self.model)\n",
    "                cost = ce + self.kl_weight * kl\n",
    "                self.optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += cost.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "            \n",
    "                # Logique d'arrêt anticipé\n",
    "            if average_loss < best_loss - self.tol:\n",
    "                best_loss = average_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= self.max_epochs_no_improve:\n",
    "                print(f\"Training loss did not improve more than tol={self.tol} for {self.max_epochs_no_improve} consecutive epochs. Stopping.\")\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            y_pred_tensor = self.model(X_tensor)\n",
    "            _, predicted = torch.max(y_pred_tensor.data, 1)\n",
    "            #predicted += 1\n",
    "            return predicted.cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3c868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fixer les graines\n",
    "#import random\n",
    "#np.random.seed(1)\n",
    "#random.seed(1)\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "# Assurer un comportement déterministe dans PyTorch (important si vous utilisez un GPU)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def objective(trial, X_train_scaled, y_train, X_val_scaled, y_val, n_epochs=1000):\n",
    "    # Suggérer des hyperparamètres\n",
    "    n_layers = trial.suggest_int('n_layers', 4, 6)\n",
    "    n_neurons = trial.suggest_int('n_neurons', 1, 16)\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-6, 1e-3, log=True)\n",
    "    kl_weight = trial.suggest_float(\"kl_weight\", 0.01, 0.1, log=True)\n",
    "\n",
    "    model_bnn = create_bayesian_network(n_layers, n_neurons, in_features=X_train_scaled.shape[1], out_features=len(np.unique(y_train_encoded)))\n",
    "    bayesian_nn = BayesianNNWrapper(model_bnn, n_epochs, batch_size, learning_rate_init, kl_weight, alpha, tol=0.0001, max_epochs_no_improve=30)\n",
    "\n",
    "    # Entraîner le modèle avec les données d'origine\n",
    "    bayesian_nn.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "    # Évaluer le modèle\n",
    "    y_pred = bayesian_nn.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Configuration d'Optuna et lancement de l'optimisation\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "study_bn = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study_bn.optimize(lambda trial: objective(trial, X_train_scaled, y_train_encoded, X_val_scaled, y_val_encoded, n_epochs=1000), n_trials=200)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Number of finished trials: {}\".format(len(study_bn.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study_bn.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0da972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fixer les graines\n",
    "#import random\n",
    "#np.random.seed(1)\n",
    "#random.seed(1)\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "# Assurer un comportement déterministe dans PyTorch (important si vous utilisez un GPU)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Récupérer les meilleurs hyperparamètres\n",
    "bn_best_params = study_bn.best_params\n",
    "\n",
    "# Créer le modèle bayésien avec les meilleurs hyperparamètres\n",
    "bayesian_model = create_bayesian_network(n_layers=bn_best_params['n_layers'], \n",
    "                                         n_neurons=bn_best_params['n_neurons'], \n",
    "                                         in_features=X_train_scaled.shape[1], \n",
    "                                         out_features=len(np.unique(y_train_encoded)))\n",
    "\n",
    "bayesian_nn = BayesianNNWrapper(\n",
    "    model=bayesian_model,\n",
    "    n_epochs=1000, \n",
    "    batch_size=bn_best_params['batch_size'],\n",
    "    learning_rate_init=bn_best_params['learning_rate_init'],\n",
    "    kl_weight=bn_best_params['kl_weight'],\n",
    "    alpha=bn_best_params['alpha'],\n",
    "    tol=0.0001, \n",
    "    max_epochs_no_improve=30\n",
    ")\n",
    "# Étape 2: Entraîner le modèle sur les données d'entraînement\n",
    "bayesian_nn.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Étape 3: Évaluer le modèle sur les données de validation\n",
    "y_pred = bayesian_nn.predict(X_val_scaled)\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "print(\"Accuracy on validation data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e4e7a",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf093c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_full_train = datrain['y']\n",
    "#  datos completos X_full_train_encoded_scaled, y_full_train\n",
    "# Création d'un objet LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Adapter le LabelEncoder aux données et transformer les étiquettes pour commencer à 0\n",
    "y_full_train_encoded = label_encoder.fit_transform(y_full_train)\n",
    "\n",
    "# Créer le modèle bayésien avec les meilleurs hyperparamètres\n",
    "bayesian_model = create_bayesian_network(n_layers=bn_best_params['n_layers'], \n",
    "                                         n_neurons=bn_best_params['n_neurons'], \n",
    "                                         in_features=X_full_train_encoded_scaled.shape[1], \n",
    "                                         out_features=len(np.unique(y_train)))\n",
    "\n",
    "full_bayesian_wrapper = BayesianNNWrapper(\n",
    "    model=bayesian_model,\n",
    "    n_epochs=1000, \n",
    "    batch_size=bn_best_params['batch_size'],\n",
    "    learning_rate_init=bn_best_params['learning_rate_init'],\n",
    "    kl_weight=bn_best_params['kl_weight'],\n",
    "    alpha=bn_best_params['alpha'],\n",
    "    tol=0.0001, \n",
    "    max_epochs_no_improve=30\n",
    ")\n",
    "# Étape 2: Entraîner le modèle sur les données d'entraînement\n",
    "full_bayesian_wrapper.fit(X_full_train_encoded_scaled, y_full_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdd34b",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352475f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "\n",
    "#X_test_encoded_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "y_pred_test = full_bayesian_wrapper.predict(X_test_encoded_scaled)\n",
    "y_pred_original_test = label_encoder.inverse_transform(y_pred_test)\n",
    "\n",
    "bnn_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_pred_original_test.flatten()})\n",
    "bnn_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7709fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "bnn_model_soumission.to_csv(pa + \"bnn_model_soumission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723cda5",
   "metadata": {},
   "source": [
    "# STACKING 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87215f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# staking2 3 modeles avec 100 trials ## \n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def objective_stacking(trial):\n",
    "    # Suggérer des hyperparamètres pour le méta-modèle\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "\n",
    "    # Créer le méta-modèle avec les hyperparamètres suggérés\n",
    "    meta_model_2 = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "    \n",
    "    # Ajout du modèle bayésien à la liste des modèles pour le stacking\n",
    "    estimators_2 = [('model_catboost', model), ('rn_best', rn_best), ('bayesian_nn', bayesian_nn)]\n",
    "    \n",
    "    stacking_2 = StackingClassifier(estimators=estimators_2, final_estimator=meta_model_2)\n",
    "    \n",
    "    stacking_2.fit(X_train_scaled, y_train_encoded)\n",
    "    y_pred = stacking_2.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "sampler_stacking = TPESampler(seed=1)\n",
    "study_stacking_2 = optuna.create_study(direction=\"maximize\", sampler=sampler_stacking)\n",
    "study_stacking_2.optimize(objective_stacking, n_trials=100)  # Augmenter le nombre d'essais si nécessaire\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study_stacking_2.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial_stacking = study_stacking_2.best_trial\n",
    "print(\"  Value (accuracy): {}\".format(trial_stacking.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial_stacking.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094eb98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Récupérer les meilleurs hyperparamètres de notre étude Optuna\n",
    "stk2_best_params = study_stacking_2.best_params\n",
    "\n",
    "learning_rate_best = stk2_best_params[\"learning_rate\"]\n",
    "n_estimators_best = stk2_best_params[\"n_estimators\"]\n",
    "max_depth_best = stk2_best_params[\"max_depth\"]\n",
    "\n",
    "# 2. Entraîner le modèle de stacking avec ces hyperparamètres\n",
    "meta_model_best_2 = GradientBoostingClassifier(learning_rate=learning_rate_best, \n",
    "                                             n_estimators=n_estimators_best, \n",
    "                                             max_depth=max_depth_best)\n",
    "\n",
    "estimators_best_2 = [('model_catboost', model), ('rn_best', rn_best), ('bayesian_nn', bayesian_nn)]\n",
    "\n",
    "stacking_best_2 = StackingClassifier(estimators=estimators_best_2, final_estimator=meta_model_best_2)\n",
    "stacking_best_2.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# 3. Valider sur l'ensemble de validation\n",
    "y_pred_val = stacking_best_2.predict(X_val_scaled)\n",
    "accuracy_val = accuracy_score(y_val_encoded, y_pred_val)\n",
    "print(\"Accuracy on validation set:\", accuracy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f84c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk2_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8afd9",
   "metadata": {},
   "source": [
    "# entraînement données complètes datrain pour prédire avec datest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1beefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_stacking_best_2 = StackingClassifier(estimators=estimators_best_2, final_estimator=meta_model_best_2)\n",
    "full_stacking_best_2.fit(X_full_train_encoded_scaled, y_full_train_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc0d07",
   "metadata": {},
   "source": [
    "# soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions dans la base de données datest\n",
    "\n",
    "y_pred_test_stacking_2 = full_stacking_best_2.predict(X_test_encoded_scaled)\n",
    "y_pred_original_test_stacking_2 = label_encoder.inverse_transform(y_pred_test_stacking_2)\n",
    "\n",
    "stacking_2_model_soumission = pd.DataFrame({'id': datest['id'], 'y': y_pred_original_test_stacking_2.flatten()})\n",
    "stacking_2_model_soumission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_2_model_soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = \"C:/Users/a/Desktop/MAITRISE HEC/SESSION_4_AUTOMNE_2023/DATA MINING/documents_etudiants_A2023/documents_etudiants_A2023/travail_equipe_fichiers_etudiants/remise_automne_2023/soumission_kaggle/\"\n",
    "stacking_2_model_soumission.to_csv(pa + \"stacking_2_model_soumission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
